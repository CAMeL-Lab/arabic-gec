{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee4cfb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80b7541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import GECTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ba329bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from rules import Rule, create_rules, annotate_data_with_rules\n",
    "import numpy as np\n",
    "from subwords_aligner import Aligner\n",
    "import re\n",
    "import json\n",
    "\n",
    "aligner = Aligner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b00ee8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GECTokenizer('/scratch/ba63/BERT_models/bert-base-arabic-camelbert-msa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51b1e06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset('/scratch/ba63/gec/data/alignment/modeling_areta_tags_improved/'\\\n",
    "                        'qalb14/qalb14_train.areta+.txt')\n",
    "\n",
    "tune_dataset =  Dataset('/scratch/ba63/gec/data/alignment/modeling_areta_tags_improved/'\\\n",
    "                        'qalb14/qalb14_tune.areta+.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae62be70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating rules...\n",
      "Done!\n",
      "There are 5060 unique rules!\n"
     ]
    }
   ],
   "source": [
    "rules = create_rules(train_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfdf6e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnt = 0\n",
    "# for rule in rules:\n",
    "#     if rule[1] >= 100:\n",
    "#         cnt += 1\n",
    "# cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd0715ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_ = [x[0] for x in rules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680cf9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_data = annotate_data_with_rules(train_dataset, rules_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74a761c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_data_tune = annotate_data_with_rules(dataset=tune_dataset, tokenizer=tokenizer, rules=rules_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "00dd3c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053\n"
     ]
    }
   ],
   "source": [
    "fixed_data = []\n",
    "unk_cnt = 0\n",
    "\n",
    "for sentence in annotated_data_tune:\n",
    "    fixed_example = []\n",
    "\n",
    "    for subword in sentence:\n",
    "        rule = Rule.from_str(subword['rule'])\n",
    "\n",
    "        if rule.edits != [['UNK']]:\n",
    "\n",
    "            if subword['subword_model'].startswith('##'):\n",
    "                fixed_example.append(f\"##{rule.apply(subword['subword'])}\")\n",
    "                \n",
    "            elif subword['subword'].startswith(' '):\n",
    "                fixed_example.append(f\" {rule.apply(subword['subword'])}\")\n",
    "                \n",
    "            else:\n",
    "                fixed_example.append(rule.apply(subword['subword']))\n",
    "\n",
    "        else:\n",
    "            unk_cnt += 1\n",
    "            if subword['subword_model'].startswith('##'):\n",
    "                fixed_example.append(f\"##{subword['subword']}\")\n",
    "                \n",
    "            elif subword['subword'].startswith(' '):\n",
    "                fixed_example.append(f\" {subword['subword']}\")\n",
    "            \n",
    "            else:\n",
    "                fixed_example.append(subword['subword'])\n",
    "\n",
    "    fixed_data.append(fixed_example)\n",
    "\n",
    "print(unk_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db7743d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, path):\n",
    "    with open(path, 'w') as f:\n",
    "        for ex in data:\n",
    "            for x in ex:\n",
    "                f.write(f\"{x['subword_model']}\\t{x['rule']}\\n\")\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e8e4500",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(annotated_data_tune, 'xyz.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cca3550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "695df42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(seq):\n",
    "    detokenize = []\n",
    "    for token in seq:\n",
    "        if token.startswith('##'):\n",
    "            detokenize[-1] += token.replace('##','')\n",
    "        \n",
    "        elif token.startswith(' '):\n",
    "            detokenize[-1] += token.strip()\n",
    "    \n",
    "        else:\n",
    "            detokenize.append(token)\n",
    "    \n",
    "    detokenize = ' '.join(detokenize)\n",
    "    detokenize = re.sub(' +', ' ', detokenize)\n",
    "    return detokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895d958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8d1aeadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenized_data = [detokenize(seq) for seq in fixed_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d40af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3f3deb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('qalb14_tunes.preds.100.txt', 'w') as f:\n",
    "    f.write('\\n'.join(detokenized_data))\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a5d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gec",
   "language": "python",
   "name": "gec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
