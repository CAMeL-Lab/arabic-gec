{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58c0e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73997737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import difflib\n",
    "import functools\n",
    "import json\n",
    "import unicodedata\n",
    "import copy\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import numpy as np\n",
    "from utils import Dataset\n",
    "from corruption_model_morph import CorruptModel, Rule, WordCorruptModel\n",
    "import torch\n",
    "\n",
    "from error_tagger.tagger import Model, inference_single\n",
    "from error_tagger.utils import ErrorTagDataset\n",
    "from transformers import pipeline\n",
    "from camel_tools.utils.charsets import UNICODE_PUNCT_SYMBOL_CHARSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8135e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_error_mle_model(edit_corrupt_model):\n",
    "    \"\"\"\n",
    "    Models P(error_tag | morph_feat)\n",
    "    \"\"\"\n",
    "    joint_counts = edit_corrupt_model.counts\n",
    "    # taking out split and insert counts\n",
    "    joint_counts = {k: v for k, v in joint_counts.items() if (k[0] != 'SPLIT' and 'INSERT' not in k[0])}\n",
    "    \n",
    "    morph_counts = dict()\n",
    "\n",
    "    for k, v in joint_counts.items():\n",
    "        morph_counts[k[1]] = v + morph_counts.get(k[1], 0)\n",
    "\n",
    "\n",
    "    assert sum(joint_counts.values()) == sum(morph_counts.values())\n",
    "\n",
    "    lookup = defaultdict(list)\n",
    "\n",
    "    for key in joint_counts:\n",
    "        prob = joint_counts[key] / morph_counts[key[1]]\n",
    "        lookup[key[1]].append((key[0], prob))\n",
    "    \n",
    "    return lookup\n",
    "\n",
    "\n",
    "\n",
    "def build_error_mle_model_full(data):\n",
    "    \"\"\"\n",
    "    Models P(error_tag | word, morph_feat)\n",
    "    \"\"\"\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    word_ana_counts = dict()\n",
    "    \n",
    "    for example in data.examples:\n",
    "        tgt_tokens = example.tgt_tokens\n",
    "        areta_tags = example.areta_tags\n",
    "        anas = example.morph_feats\n",
    "        \n",
    "        for token, tag, ana in zip(tgt_tokens, areta_tags, anas):\n",
    "            \n",
    "            if tag != 'UC' and tag != 'UNK':\n",
    "                if tag == 'SPLIT' and len(token.split()) == 2:\n",
    "                    tokens = token.split()\n",
    "                    ana_str = json.dumps([ana[0]], ensure_ascii=True)\n",
    "                    model[(tokens[0], ana_str)][tag] += 1\n",
    "                    word_ana_counts[(tokens[0], ana_str)] = 1 + word_ana_counts.get((tokens[0], ana_str), 0)\n",
    "\n",
    "                elif tag != 'SPLIT':\n",
    "                    ana_str = json.dumps(ana, ensure_ascii=True)\n",
    "                    model[(token, ana_str)][tag] += 1\n",
    "                    word_ana_counts[(token, ana_str)] = 1 + word_ana_counts.get((token, ana_str), 0)\n",
    "    \n",
    "    \n",
    "    for token, ana in model:\n",
    "        for tag in model[(token, ana)]:\n",
    "            model[(token, ana)][tag] /= word_ana_counts[(token, ana)]\n",
    "    \n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfa776aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset(raw_data_path='/scratch/ba63/gec/data/alignment/modeling_areta_tags_check/qalb14/'\\\n",
    "               'corruption_data/mix_train.areta.txt',\n",
    "              morph_feats_path='/scratch/ba63/gec/data/alignment/modeling_areta_tags_check/qalb14/'\\\n",
    "               'corruption_data/mix_train_morph.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7775f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_corrupt_model = CorruptModel.build(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de9a6f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_corrupt_model = WordCorruptModel.build(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e55f0d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_mle_model_full = build_error_mle_model_full(data)\n",
    "# error_mle_model = build_error_mle_model(edit_corrupt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4be6d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('error_tagger/model_w_morph.config.json') as f:\n",
    "    model_config = json.load(f)\n",
    "error_nn_model = Model(**model_config)\n",
    "error_nn_model.load_state_dict(torch.load('error_tagger/model_w_morph.pt'))\n",
    "vectorizer = ErrorTagDataset.load_vectorizer('error_tagger/vectorizer.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84ba46f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit Corruption Model Stats\n",
      "--------------------\n",
      "Total number of (error tag, morph_feat) pairs: 13595\n",
      "Total number of rules: 42055\n",
      "\n",
      "\n",
      "Word Corruption Model Stats\n",
      "--------------------\n",
      "Total number of (error tag, morph_feat, word) mappings: 81892\n"
     ]
    }
   ],
   "source": [
    "keys = list(edit_corrupt_model.model.keys())\n",
    "areta_tags = set([x[0] for x in keys])\n",
    "\n",
    "total_rules = sum([len(edit_corrupt_model[key]) for key in edit_corrupt_model.model.keys()])\n",
    "counts_by_key = {key: sum([edit_corrupt_model[key][x] for x in edit_corrupt_model[key]]) for key in keys}\n",
    "counts_by_key = sorted(counts_by_key.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "print(f'Edit Corruption Model Stats')\n",
    "print('--------------------')\n",
    "print(f'Total number of (error tag, morph_feat) pairs: {len(keys)}')\n",
    "print(f'Total number of rules: {total_rules}')\n",
    "print()\n",
    "print()\n",
    "print(f'Word Corruption Model Stats')\n",
    "print('--------------------') \n",
    "print(f'Total number of (error tag, morph_feat, word) mappings: {len(word_corrupt_model.model.keys())}')\n",
    "# print(f'Rules: ')\n",
    "# counts_by_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298dae5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error = 'INSERT_XM'\n",
    "set_m = set()\n",
    "for key in keys:\n",
    "    if error in key[0]:\n",
    "        print(key)\n",
    "        set_m.add(key[1])\n",
    "        for x in edit_corrupt_model[key]:\n",
    "            \n",
    "            print(f'Rule: {x}')\n",
    "            print(f'Count: {edit_corrupt_model[key][x]}')\n",
    "            print(f'Example: {edit_corrupt_model.examples[key][x][0]}')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb17603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error_tags = list(edit_corrupt_model.model.keys())\n",
    "\n",
    "# error_probs = {error: edit_corrupt_model.counts[error]/total_error_counts for error in error_tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25459ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_errors(tokens, morph_feats, error_prop, std_dev,\n",
    "                     edit_corruption_model,\n",
    "                     word_corruption_model,\n",
    "                     error_mle_model,\n",
    "                     error_nn_model,\n",
    "                     vectorizer,\n",
    "                     op_probs=None):\n",
    "    \n",
    "    \n",
    "\n",
    "    num_errors = int(np.round(np.random.normal(error_prop, std_dev) * len(tokens)))\n",
    "    num_errors = min(max(0, num_errors), len(tokens))  # num_errors \\in [0; len(tokens)]\n",
    "    \n",
    "    corruptions_tags = []\n",
    "    \n",
    "    if num_errors == 0:\n",
    "        return ' '.join(tokens), corruptions_tags\n",
    "    \n",
    "    \n",
    "    token_ids_to_modify = np.random.choice(len(tokens), num_errors, replace=False)\n",
    "\n",
    "\n",
    "    new_sentence = []\n",
    "\n",
    "    fill_mask = pipeline('fill-mask', model='/scratch/ba63/gec/mlm', top_k=1)\n",
    "\n",
    "    for token_id in range(len(tokens)):\n",
    "        if token_id not in token_ids_to_modify:\n",
    "            new_sentence.append(tokens[token_id])\n",
    "            continue\n",
    "        \n",
    "\n",
    "        current_token, current_feat = tokens[token_id], morph_feats[token_id]\n",
    "        str_feat = json.dumps(current_feat, ensure_ascii=False)\n",
    "        \n",
    "        operation = np.random.choice(['replace', 'insert', 'delete'], p=[0.85, 0.14, 0.01])\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(f'Correct word: {current_token}')\n",
    "#         print(f'Morph Feat: {str_feat}')\n",
    "#         print(f'Sampled error: {operation}')\n",
    "    \n",
    "        if operation == 'replace':\n",
    "            compatible_tags, compatible_rules = get_compatible_tags(current_token, str_feat,\n",
    "                                                                    edit_corruption_model)\n",
    "\n",
    "\n",
    "            if len(compatible_tags) > 0:\n",
    "                \n",
    "                \n",
    "                print()\n",
    "                # getting the mle tags\n",
    "                mle = error_mle_model.get((current_token, str_feat), None)\n",
    "\n",
    "                # getting the tags that are only compatible\n",
    "                mle_tags = []\n",
    "                if mle != None:\n",
    "                    mle_tags = [(tag, prob) for tag, prob in mle.items() if tag in compatible_tags]\n",
    "                    mle_tags = sorted(mle_tags, key=lambda x: x[-1], reverse=True)\n",
    "\n",
    "                # getting the nn tags\n",
    "                nn = inference_single(error_nn_model, vectorizer, current_token, current_feat)\n",
    "                nn_tags = [(tag, prob) for tag, prob in nn['top5'] if tag in compatible_tags]\n",
    "\n",
    "                \n",
    "                if len(nn_tags) == 0 and len(mle_tags) == 0:\n",
    "                    tag = 'OOV'\n",
    "#                     print(\"CANNOT CORRUPT\")\n",
    "\n",
    "                elif len(mle_tags) == 0 and len(nn_tags) != 0:\n",
    "                    tag = max(nn_tags, key=lambda x: x[-1])[0]\n",
    "\n",
    "                elif len(nn_tags) == 0 and len(mle_tags) != 0:\n",
    "                    tag = max(mle_tags, key=lambda x: x[-1])[0]\n",
    "\n",
    "                else:\n",
    "                    max_mle_tag = max(mle_tags, key=lambda x: x[-1])\n",
    "                    max_nn_tag = max(nn_tags, key=lambda x: x[-1])\n",
    "                    tag = max_mle_tag[0] if max_mle_tag[1] > max_nn_tag[1] else  max_nn_tag[0]\n",
    "\n",
    "                # special case for pnx, fix later to make prettier\n",
    "                if current_token in UNICODE_PUNCT_SYMBOL_CHARSET:\n",
    "                    if 'INSERT_PM' in mle_tags and 'REPLACE_PC' in mle_tags:\n",
    "                        tag = np.random.choice(['INSERT_PM', 'REPLACE_PC'], p=[0.5, 0.5])\n",
    "                \n",
    "#                 print(f'MLE tags: {mle_tags}\\n')\n",
    "#                 print(f'NN tags: {nn_tags}\\n')\n",
    "#                 print(f'Corruption tag: {tag}\\n')\n",
    "                corruptions_tags.append(tag)\n",
    "        \n",
    "                # corruption\n",
    "\n",
    "                if tag != 'OOV':\n",
    "                    global_corruptions = word_corruption_model[(tag, str_feat, current_token)]\n",
    "                    if len(global_corruptions) != 0:\n",
    "\n",
    "                        corrupted = max(global_corruptions, key=global_corruptions.get)\n",
    "#                         print(f'Rule: Global Corruption Model\\n')\n",
    "                        \n",
    "                    else:\n",
    "                        # getting the compatible rules\n",
    "                        rules = compatible_rules[(tag, str_feat)]\n",
    "                        max_rule = Rule.from_str(max(rules, key=rules.get))\n",
    "                        corrupted = max_rule.apply(current_token)\n",
    "#                         print(f'Rule: {max_rule.to_str()}\\n')\n",
    "                    \n",
    "                    if corrupted is None:\n",
    "                        assert tag == 'SPLIT'\n",
    "                        corrupted = current_token\n",
    "                        new_sentence.append(current_token+'_SPLIT')\n",
    "                    else:\n",
    "                        new_sentence.append(corrupted)\n",
    "                    \n",
    "                else:\n",
    "                    corrupted = current_token\n",
    "                    new_sentence.append(corrupted)\n",
    "                    \n",
    "#                 print(f'Corrupted word: {corrupted}')\n",
    "                    \n",
    "\n",
    "\n",
    "#             else:\n",
    "#                 print(f'No compatible tags for selected words')\n",
    "            \n",
    "        elif operation == 'delete':\n",
    "#             print(f'Corrupted word: {current_token}')\n",
    "#             print(f'Corruption tag: DELETE')\n",
    "        \n",
    "            corruptions_tags.append('DELETE')\n",
    "        \n",
    "        elif operation == 'insert':\n",
    "#             print(f'Corrupted word: [MASK]')\n",
    "            new_sentence.append(current_token)\n",
    "            new_sentence.append('[MASK]')\n",
    "            corruptions_tags.append('INSERT')\n",
    "        \n",
    "            \n",
    "        print('----------------------------------------')\n",
    "    \n",
    "    new_sentence = ' '.join(new_sentence)\n",
    "    \n",
    "    # applying split errors\n",
    "    if '_SPLIT' in new_sentence:\n",
    "        new_sentence = re.sub(r'\\_SPLIT\\s', '', new_sentence)\n",
    "    \n",
    "    # filling the masked tokens\n",
    "    # we will ignore masked positions where the prediction is a\n",
    "    # subword\n",
    "    if '[MASK]' in new_sentence:\n",
    "        filled_masks = fill_mask(new_sentence)\n",
    "        if len(filled_masks) > 1:\n",
    "            masked_tokens = [x[0]['token_str'] for x in filled_masks]\n",
    "        else:\n",
    "            masked_tokens = [filled_masks[0]['token_str']]\n",
    "        mask_idx = 0\n",
    "        sentence_tokens = new_sentence.split(' ')\n",
    "        sentence_tokens_ = []\n",
    "\n",
    "        for i in range(len(sentence_tokens)):\n",
    "            if sentence_tokens[i] == '[MASK]':\n",
    "                masked_token = masked_tokens[mask_idx]\n",
    "                if '##' not in masked_token:\n",
    "                    sentence_tokens_.append(masked_token)\n",
    "                mask_idx += 1\n",
    "            else:\n",
    "                sentence_tokens_.append(sentence_tokens[i])\n",
    "\n",
    "        assert '[MASK]' not in sentence_tokens_\n",
    "        new_sentence = ' '.join(sentence_tokens_)\n",
    "\n",
    "    \n",
    "    new_sentence = re.sub(r' +', ' ', new_sentence)\n",
    "    \n",
    "    return new_sentence, corruptions_tags\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "def get_compatible_tags(word, morph_feats, model):\n",
    "\n",
    "    compatible_tags = set()\n",
    "    compatible_rules = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    for key in model.model.keys():\n",
    "        if key[1] == morph_feats and 'DELETE' not in key[0]:\n",
    "            for rule in model[key]:\n",
    "                rule_obj = Rule.from_str(rule)\n",
    "                if rule_obj.is_applicable(word):\n",
    "                    compatible_tags.add(key[0])\n",
    "                    compatible_rules[key][rule] = model[key][rule]\n",
    "\n",
    "    return list(compatible_tags), compatible_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da841521",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = Dataset(raw_data_path='/scratch/ba63/gec/data/alignment/modeling_areta_tags_check/qalb14/'\\\n",
    "               'corruption_data/qalb14_tune.areta.txt',\n",
    "              morph_feats_path='/scratch/ba63/gec/data/alignment/modeling_areta_tags_check/qalb14/'\\\n",
    "               'corruption_data/qalb14_tune_morph.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccac30a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_sents_idx = []\n",
    "# we need to handle the more than one token problem!\n",
    "for i in range(len(dev_dataset.examples)):\n",
    "    tgt_tokens = dev_dataset.examples[i].tgt_tokens\n",
    "    if sum([len(x.split()) for x in tgt_tokens]) == len(tgt_tokens) and 40 <= len(tgt_tokens) <= 60 :\n",
    "        testing_sents_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c4ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = dev_dataset.examples[12].tgt_tokens\n",
    "# morph_feats = dev_dataset.examples[12].morph_feats\n",
    "error_prop=0.20\n",
    "std=0.2\n",
    "# print(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba1e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tags = []\n",
    "corrupted_sentences = []\n",
    "\n",
    "for idx in testing_sents_idx[:20]:\n",
    "    tokens = dev_dataset.examples[idx].tgt_tokens\n",
    "    morph_feats = dev_dataset.examples[idx].morph_feats\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    corrupted, corruption_tags  = introduce_errors(tokens=tokens,\n",
    "                                                 morph_feats=morph_feats,\n",
    "                                                 error_prop=error_prop,\n",
    "                                                 std_dev=std,\n",
    "                                                 edit_corruption_model=edit_corrupt_model,\n",
    "                                                 word_corruption_model=word_corrupt_model,\n",
    "                                                 error_mle_model=error_mle_model_full,\n",
    "                                                 error_nn_model=error_nn_model,\n",
    "                                                 vectorizer=vectorizer)\n",
    "\n",
    "    corrupted_sentences.append((' '.join(tokens), corrupted))\n",
    "    print('**************************************************')\n",
    "    print('**************************************************')\n",
    "    \n",
    "    print(' '.join(tokens))\n",
    "    print(corrupted)\n",
    "    \n",
    "    print('**************************************************')\n",
    "    print('**************************************************')\n",
    "    total_tags.extend(corruption_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03530365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags_cnt = Counter(total_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a7ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0942bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in corrupted_sentences:\n",
    "    correct = sent[0]\n",
    "    corrupted = sent[1]\n",
    "    \n",
    "    if correct != corrupted:\n",
    "        print(correct)\n",
    "        print(corrupted)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8430caaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febff9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc9fb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c47ec23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76122094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8846ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (misc)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
